Overview
--------

- I'm a compiler engineer at Embecosm
    - where I mainly work on GNU toolchains.
- I'm interested in Numba because I used and developed it in my previous job
    - at Continuum Analytics.
    - I spent time with customers, showing them how to get the best out of it
    - and that fed back into improvements in Numba itself.
- My personal interests generally lie in the intersection of:
    - Python
    - High performance computing
    - and Numerical methods
- and Numba is one particular tool that lies in that intersection.


- My plan for this talk is to give an introduction to Numba
    - What it is, and;
    - what it does
- But mostly I'd like to focus on talking about how it works
    - a large part of that is using LLVM through llvmlite

- and I'm also interested in your feedback and thoughts on Numba and llvmlite
    - and what we could do better or differently.


What is Numba? (1)
------------------

- Python isn't regarded as one of the fastest languages
    - Numba is a tool that you can use to make python code run faster,
    - by compiling it.
- It works best for array-oriented and numerical codes, because that's where
  most of the development effort has been focused.
- Code that uses a lot of object-oriented design and very dynamic features of
  Python aren't going to work so well with Numba.
- You might want to use Numba as an alternative to other mechanisms for using
  native code with Python
    - like using the C API or CFFI to interface with code in another language

- The benefit of using Numba over these alternatives
    - is in keeping all your source in one language
    - which is a lot easier for developers to maintain,
    - so it can keep development effort down.


What is Numba? (2)
------------------

- Numba doesn't do anything unless you tell it to
- that is, you have to tell it which Python functions you want to compile
    - Unlike PyPy or V8, it doesn't get applied to a whole program
    - and it isn't a tracing JIT - it always compiles code before executing it
- One of the reasons it's opt-in is that there's a trade off
    - It relaxes some semantics of Python code in return for better performance
- This is a narrower focus than some other projects
    - which allows us to handle CPU and non-CPU targets with good performance
    - without being excessively complicated to use


Implementation overview
-----------------------

- Essentially, Numba is a just-in-time Python compiler that uses LLVM
- It targets CPUs, and CUDA or HSA GPUs
- There's several different Python interpreters
     - The most commonly-used one is CPython
     - and Numba works with the recent CPython 2 and 3 versions
- The reason for supporting CPython is that that's the interpreter that people
  who do a lot of numerical and scientific computing use
- They make heavy use of the Numpy and Scipy libraries
    - and the whole ecosystem that's built up around those
    - so a lot of effort has gone into making Numba work alongside them
      efficiently.
- It runs on the 3 main platforms and it's BSD licensed, so most people can run
  it.
- The development has been supported by Continuum Analytics, who employ
  developers that have done most of the work on Numba
- and now the Moore Foundation is also providing financial support towards
  getting Numba to a 1.0 release
    - I'll talk a bit more about that later


Who is using Numba?
-------------------

- There's a large and growing Numba userbase,
- which I believe is mainly of scientists, engineers, and people interested in
  numerical modelling of some kind
- It's difficult to get an exact number, but
    - according to PyPI stats it's been downloaded 135000 times
    - However I think the majority will have been installed through a package
      manager called Conda
    - I don't have the figures for Conda downloads though
- I don't know of a centralised list of Numba users
- but with a few minutes googling quite a few will turn up. for example:
    - software for economists, biologist, earth sciences, and,
    - kite simulation
- There's lots more examples... I should put together a list.

Numba example
-------------

- This is a brief overview of what using Numba looks like.
- Here we have a python implementation of a Mandelbrot function

- To use Numba, we import its jit decorator
- and then we "decorate" a function we want it to compile with jit

- Now when the mandel function is called, it isn't executed by the Python
  interpreter
    - Instead, Numba jumps in, compiles the function, and executes the compiled
      code.
    - In a minute I'll go into that process in a bit more detail


Mandelbrot performance
----------------------

- But for now, i'll just finish off the example by discussing its performance.
- If we consider the pure Python function running on the CPython interpreter as
  the baseline...


Other examples
--------------

- Obviously the speedup you get varies on the code you're running.
- These are a few other examples from a Numba tutorial I did a while ago
- these are all small examples
- but remember there are much larger pieces of software that use Numba
  successfully,
    - like some of the ones I mentioned a couple of slides ago.


Dispatch process
----------------

- So how does Numba actually work?
- Let's start by going over what it does at the time of a jitted function call
- then look at various parts in more detail.

When a call to a jitted function is made:

- Numba inspects the types of the arguments to the function.
- It does this because it compiles a different specialisation of the function
  for each set of input types
    - in practise you only end up with a few different compiled versions
    - usually people tend to call functions again and again with the same types.
- It caches previously-compiled versions of the function
    - so if there's an already-compiled specialisation it can use that
    - if not, it has to compile a new specialisation
- Once there's a compiled version available,
   - it marshals the Python arguments to their native types
   - executes the native function
   - and marshals the return value back into a Python object.


Dispatch overhead
-----------------

- That process usually takes longer than a simple Python function call
- So there is a little bit of overhead from using Numba
- For example, if you used it with a function that just adds two numbers
  together:
    - you'd find that the Numba version takes twice as long as the Python
      version.
    - In practice, it doesn't take a lot of computation for the speed up in the
      function body to outweigh the cost of the dispatch.


Compilation pipeline
--------------------

Let's look a bit closer at the compilation process.

- Because Numba uses LLVM, the Numba part of the compilation process is all
  concerned with dealing with Python-specific issues.
- There's two ways you could write a compiler for Python:
    - One is to get the AST for a function and use that as your starting point
    - Another is to get the bytecode for the function, and use that as a start

- Early versions of Numba used the AST.
    - But that can be problematic as you don't always have the Python source
      code and AST available in all circumstances, so it can fail.
    - Even when you do have the AST, the semantics of what you have to do with
      it seem more complicated to me.
- So now, Numba uses the Python bytecode.
- It takes the bytecode and does a few things to turn it into a Numba IR
    - which is an SSA representation
    - but it has no type information yet
- So next the type inference phase runs to assign a type to every value in the
  IR
- Then it becomes a relatively straightforward process to translate the Typed
  Numba IR into LLVM IR
- And everything beyond that can be handled by LLVM
- lots of tricky stuff that you guys have already sorted out, that we don't want
  to deal with.


Type inference
--------------

- Before I go on to the LLVM interface, this is a quick look at type inference
- Python code isn't statically typed, but LLVM IR is.
- Numba has a fairly simple type inference mechanism
    - that adds this information to the Numba IR
- it uses two things:
    - one is a set of mappings from input types to output types for every
      operation
    - this is contained within numba

